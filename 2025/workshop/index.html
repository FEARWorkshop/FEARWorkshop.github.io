<!DOCTYPE html>
<html lang="en-gb">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
        Formal Ethical Agents and Robots — FEAR 2025
    </title>
    <meta name="description"
        content="Information about the Formal Ethical Agents and Robots workshop (FEAR) 2024.">

    <link rel="stylesheet" type="text/css" href="/styles/dropdown.css" />
    <link rel="stylesheet" type="text/css" href="/styles/fonts.css" />
    <link rel="stylesheet" type="text/css" href="/styles/highlight.css" />
    <link rel="stylesheet" type="text/css" href="/styles/main.css" />
    <link rel="stylesheet" type="text/css" href="/styles/navbar.css" />
    <link rel="stylesheet" type="text/css" href="/styles/normalize.css" />
    <link rel="stylesheet" type="text/css" href="/styles/program.css" />
    <link rel="stylesheet" type="text/css" href="/styles/speakers.css" />
    <link rel="stylesheet" type="text/css" href="/styles/tables.css" />
    <link rel="stylesheet" type="text/css" href="/styles/2025/main.css" />
    <link rel="preload" fetchpriority="high" as="image" href="/media/2025/nancy_rothwell.jpeg"
        type="image/jpeg">

    <!--
    Does nothing, but prevents flash of unstyled content in Firefox.
    Thanks: https://stackoverflow.com/a/64158043
  -->
    <script>
        let fix_FOUC_in_firefox;
    </script>

</head>

<body>
    <div class="fancy" id="header">
        <div id="nav-container">
            <div id="navbar" role="navigation">
                <a href="/2025/">Home</a>
                <!-- <a href="#schedule-and-location">Venue</a> -->
                <!-- <a href="#program">Program</a> -->
                <a href="#keynote-speaker">Speakers</a>
                <a href="#submission-information">Submission</a>

                <label id="menu-dropdown">
                    <div id="dropdown-button">Committees</div>
                    <input id="dropdown-input" type="checkbox" />
                    <ul id="dropdown-menu">
                        <li><a href="#program-chair">Program Chairs</a></li>
                        <li><a href="#program-committee">Program Committees</a></li>
                    </ul>
                </label>
                <a href="/2025/tutorial/">Tutorial</a>
            </div>
        </div>

        <div id="title-block">
            <h1><span><a href="/2025/" title="Go to home page">FEAR 2025<span style="font-size: 0.8em"> </span></a></span>
            </h1>

            <div id="subtitle"><span>3 - 4 Nov, Manchester, UK</span></div>
        </div>
    </div>

    <div id="doc" role="main">
        <h2 id="welcome" class="section"><a class="anchor" href="#welcome"
            title="permalink to this section">Welcome to FEAR 2025</a></h2>
        <p>The Second International Workshop on Formal Ethical Agents and Robots (FEAR) will be held on 4 November 2025 
            with co-located tutorial scheduled for 3 November 2025 both at the Nancy Rothwell Building, the University of Manchester, UK.</p>

        <h2 id="overview" class="section"><a class="anchor" href="#overview"
            title="permalink to this section">Overview</a></h2>
        <p>Recent advances in artificial intelligence have led to a range of concerns about the ethical impact of the
            technology. This includes concerns about the day-to-day behaviour of robotic systems that will interact with
            humans in workplaces, homes and hospitals. One of the themes of these concerns is the need for such systems
            to take ethics into account when reasoning. This has generated new interest in how we can specify, implement
            and validate ethical reasoning. The aim of this workshop would be to look at formal approaches to these
            questions. Topics of interest include but are not limited to:</p>
        <ul>
            <li>Logics for morality and ethics</li>
            <li>Knowledge representation of ethical theories and ethically salient information</li>
            <li>Computational modelling of morality and ethics</li>
            <li>Specification of ethical reasoning and behaviour</li>
            <li>Verification of ethical reasoning and behaviour</li>
            <li>Formal modelling of ethical accountability</li>
        </ul>
        <h2 id="schedule-and-location" class="section"><a class="anchor" href="#schedule-and-location"
            title="permalink to this section">Schedule and Location</a></h2>
        <p>This year we will hold a one-day workshop with informal proceedings, consisting of short talks and opportunities for
            discussion, and a one-day tutorial that engages conversation in the state-of-the-art machine ethics fields. 
            The workshop and tutorial will be held at Nancy Rothwell Building, the University of Manchester.
            <br><br>
            FEAR will be a hybrid meeting, supporting both virtual and in-person attendance. We strongly encourage in-person attendance.
            Registration for the workshop is now open via this <a href=""#>link</a>. 
        </p>
        <!-- <h2 id="workshop-location" class="section"><a class="anchor" href="#workshop-location"
                title="permalink to this section">Workshop Location</a></h2>
        <p>The workshop will be in the Nancy Rothwell Building, the University of Manchester.</p> -->
        <!-- <h2 id="program" class="section"><a class="anchor" href="#program" title="permalink to this section">Program —
                Monday, <span style="white-space: nowrap">11 Nov 2024</span></a></h2>
        <div class="program" paragraphs="none">
            <div class="day">
                <div class="item">
                    <div class="time">11:00–12:30</div>
                    <div class="item-text">Workshop participants are invited to attend the FMAS Keynote talk by Silvia
                        Lizeth Tapia Tarifa.</div>
                </div>
                <div class="item">
                    <div class="time">12:30–13:30</div>
                    <div class="item-text">Lunch</div>
                </div>
                <div class="item">
                    <div class="time">13:30–14:30</div>
                    <div class="item-text">Keynote Talk
                        <div><a class="extlink " href="#marija-slavkovik-speaker-info"><strong>Understanding Privacy by
                                    Formalising It</strong></a><br /> <em>Marija Slavkovik, University of Bergen</em>
                        </div>
                    </div>
                </div>
                <div class="item">
                    <div class="time">14:30–15:00</div>
                    <div class="item-text">
                        <div><strong>Acting for the Right Reasons: Creating Reason-Sensitive Artificial Moral
                                Agents</strong><br /> <em>Kevin Baum et al.</em></div>
                    </div>
                </div>
                <div class="item">
                    <div class="time">15:00–15:30</div>
                    <div class="item-text">Coffee Break</div>
                </div>
                <div class="item">
                    <div class="time">15:30–16:20</div>
                    <div class="item-text">
                        <div><strong>From Responsibility, via Indifference, to Recklessness</strong><br /> <em>Michael
                                Fisher</em></div>
                        <div><strong>The Need for Formal Specifications of Morally Relevant Information for Algorithmic
                                Decision-Making</strong> <span style="white-space: nowrap">(short paper)</span><br />
                            <em>Marija Slavkovik</em></div>
                    </div>
                </div>
                <div class="item">
                    <div class="time">16:20</div>
                    <div class="item-text">Community Discussion</div>
                </div>
            </div>
        </div> -->
        <h2 id="keynote-speaker" class="section"><a class="anchor" href="#keynote-speaker"
                title="permalink to this section">Keynote Speaker</a></h2>
        <div class="invited-speaker" id="mmarina-speaker-info">
            <div class="title-name-affiliation">
                <div class="talk-title">Norm Emergence in Multiagent Systems</div>
                <div class="name-affiliation">
                    <div class="name"><a href="https://researchportal.bath.ac.uk/en/persons/marina-de-vos">Marina De Vos</a></div>
                    <div class="affiliation">University of Bath, UK</div>
                </div>
            </div>
            <div class="info-and-image">
                <div class="info-container">
                    <div class="talk-abstract" paragraphs="none"><span>Abstract</span>
                        <p>Norm emergence is typically studied in the context of multiagent systems (MAS) where norms are implicit, 
                            and participating agents use simplistic decision-making mechanisms. These implicit norms are usually 
                            unconsciously shared and adopted through agent interaction. A norm is deemed to have emerged when a 
                            threshold or predetermined percentage of agents follow the “norm”. Conversely, in normative MAS, norms 
                            are typically explicit and agents deliberately share norms through communication or are informed about 
                            norms by an authority, following which an agent decides whether to adopt the norm or not. The decision 
                            to adopt a norm by the agent can happen immediately after recognition or when an applicable situation 
                            arises. In this paper, we make the case that, similarly, a norm has emerged in a normative MAS when a 
                            percentage of agents adopt the norm. Furthermore, we posit that agents themselves can and should be 
                            involved in norm synthesis, and hence influence the norms governing the MAS, in line with Ostrom’s 
                            eight principles. Consequently, we put forward a framework for the emergence of norms within a normative 
                            MAS, that allows participating agents to propose/request changes to the normative system, while special-
                            purpose synthesizer agents formulate new norms or revisions in response to these requests. Synthesizers 
                            must collectively agree that the new norm or norm revision should proceed, and then finally be approved 
                            by an “Oracle”. The normative system is then modified to incorporate the norm.</p>
                    </div>
                    <div class="speaker-bio">
                        <figure class="headshot-container"><img src="/media/2025/marina.jpg"
                                alt="Marina De Vos" title="Marina De Vos" />
                            <figcaption><a href="https://researchportal.bath.ac.uk/en/persons/marina-de-vos">Marina De Vos</a></figcaption>
                        </figure>
                        <div class="bio-text" paragraphs="none"><span>Biography</span>
                            <p>Marina De Vos is a senior lecturer/associate professor in artificial intelligence and the director of 
                                training for the UKRI Centre for Doctoral Training in Accountable, Responsible, and Transparent AI at 
                                the University of Bath. With a strong background in automated human reasoning, Marina's research focuses 
                                on enabling improved access to specialist knowledge, the logical foundations of AI systems, explainable 
                                artificial intelligence methods, and modelling the behaviour of autonomous systems.

                                In her work on normative multi-agent systems, Marina combines her interests in the development of software 
                                tools and methods, drawing from a diverse range of domains including software verification, logic 
                                programming, legal reasoning, and AI explainability, to effectively model, verify and explain autonomous 
                                agents. Currently, Marina's exploration involves systems that possess the ability to autonomously evolve 
                                through external and internal stimuli.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div> 
        <div class="invited-speaker" id="vivek-speaker-info">
            <div class="title-name-affiliation">
                <div class="talk-title">What Virtues Should a Care Robot Possess? Some Simulation Experiments and Human Results</div>
                <div class="name-affiliation">
                    <div class="name"><a href="https://www.adaptcentre.ie/experts/vivek-nallur/">Vivek Nallur</a></div>
                    <div class="affiliation">University College Dublin, Ireland</div>
                </div>
            </div>
            <div class="info-and-image">
                <div class="info-container">
                    <div class="talk-abstract" paragraphs="none"><span>Abstract</span>
                        <p>The common consensus is that robots designed to work alongside or serve humans must adhere to the ethical standards 
                            of their operational environment. To achieve this, several methods based on established ethical theories have been 
                            suggested. Nonetheless, numerous empirical studies show that the ethical requirements of the real world are very 
                            diverse and can change rapidly from region to region. This eliminates the idea of a universal robot that can  fit 
                            into any ethical context. However, creating customised robots for each deployment, using existing techniques is 
                            challenging. This talk presents a way to overcome this challenge by introducing a virtue ethics inspired 
                            computational method that enables character-based tuning of robots to accommodate the specific ethical needs of an 
                            environment. Using a simulated elder-care environment, I will illustrate how tuning can be used to change the 
                            behaviour of a robot that interacts with an elderly resident in an ambient-assisted environment.</p>
                    </div>
                    <div class="speaker-bio">
                        <figure class="headshot-container"><img src="/media/2025/vivek.jpg"
                                alt="Marina De Vos" title="Vivek Nallur" />
                            <figcaption><a href="https://www.adaptcentre.ie/experts/vivek-nallur/">Vivek Nallur</a></figcaption>
                        </figure>
                        <div class="bio-text" paragraphs="none"><span>Biography</span>
                            <p>Dr. Vivek Nallur works on Computational Machine Ethics. He is interested in how to implement and validate ethics 
                                in autonomous machines.  What kinds of ethical decision-making can we implement? How can these be checked and 
                                reliably tuned to individual circumstances? What combinations of individually ethical decisions could lead to 
                                un-ethical behaviour? These, by nature, are inter-disciplinary questions, and he is quite interested in 
                                collaborating with folks in the field of philosophy/law/politics etc. He is also a Senior Member of the IEEE. He 
                                is a full voting member, and serve on the IEEE P7008 Standards committee for Ethically Driven Nudging for Robotic, 
                                Intelligent and Autonomous Systems. 
                                <br><br>
                                Multi-Agent Systems (MAS) are his preferred tool for approaching problems in decision-making, self-adaptation, 
                                complexity, emergence, etc. They lend themselves to extensive forms of experimentation: having all agents follow 
                                simple rules, implementing complex machine-learning algorithms, investigating the interplay of different algorithms 
                                being used at the same time, are all possible with relatively simple conceptual structures.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div> 
        <h2 id="submission-information" class="section"><a class="anchor" href="#submission-information"
                title="permalink to this section">Submission Information</a></h2>
        <p>We will accept three types of paper prepared using the EPTCS LaTeX style.</p>
        <ul class="loose-list">
            <li>Regular papers describing completed research (up to 16 pages in length).</li>
            <li>Short papers describing work-in-progress or directions for future work (up to 8 pages in length).</li>
            <li>Abstracts providing a high-level description of some research published (or submitted/intended for
                publication) elsewhere (up to 2 pages in length).</li>
        </ul>
        <p>Papers should be submitted using <a class="extlink emphasised"
                href="https://openreview.net/group?id=FEAR/2025/Workshop">OpenReview</a>.
            The paper type should be included as part of the paper title. Reviews will be single-blind. Please note that
            OpenReview can take up to two weeks to approve a new profile registration that does not contain an
            institution email address.</p>
        <p>We intend to host informal workshop proceedings on OpenReview.</p>
        <h2 id="important-dates" class="section"><a class="anchor" href="#important-dates"
                title="permalink to this section">Important Dates</a></h2>
        <ul>
            <li>Submission Deadline: Friday, 1<sup>st</sup> of August 2025, 11:59 UTC-0</li>
            <li>Notification: Friday, 12<sup>th</sup> of September 2025</li>
            <li>Workshop: Tuesday, 4<sup>th</sup> of November 2025</li>
        </ul>
        <h2 id="program-chair" class="section"><a class="anchor" href="#program-chair"
                title="permalink to this section">Program Chair</a></h2>
                <div class="one-item-list"><strong>Louise Dennis</strong>, University of Manchester</div>
                <div class="one-item-list"><strong>Marija Slavkovik</strong>, University of Bergen</div>
                <div class="one-item-list"><strong>Raynaldio Limarga</strong>, University of Manchester</div>
        <h2 id="program-committee" class="section"><a class="anchor" href="#program-committee" title="permalink to this section">Program Committee</a></h2>
        <ul>
            <li><strong>Kevin Baum</strong>, DFKI</li>
            <li><strong>Andreas Brännström</strong>, Umeå University</li>
            <!-- <li><strong>Ilaria Canavotto</strong>, University of Maryland</li> -->
            <!-- <li><strong>Sarah Christensen</strong>, University of Leeds</li> -->
            <li><strong>Joe Collenette</strong>, University of Chester</li>
            <li><strong>Alex Jackson</strong>, Kings College London</li>
            <li><strong>Aleks Knoks</strong>, University of Luxembourg</li>
            <li><strong>Simon Kolker</strong>, University of Manchester</li>
            <!-- <li><strong>Lara Lawnicza</strong>, University of Bamberg</li> -->
            <li><strong>Vivek Nallur</strong>, University College Dublin</li>
            <li><strong>Samer Nashed</strong>, University of Montreal</li>
            <li><strong>Maurice Pagnucco</strong>, University of New South Wales</li>
            <li><strong>Luca Pasetto</strong>, University of Luxembourg</li>
            <li><strong>Jazon Szabo</strong>, Kings College London</li>
            <!-- <li><strong>Rajitha Ramanayake</strong>, University College Dublin</li> -->
            <li><strong>Maike Schwammberger</strong>, Karlsruhe Institute of Technology</li>
            <li><strong>Dieter Vanderelst</strong>, University of Cincinnati</li>
            <li><strong>Gleifer Vaz Alves</strong>, Federal University of Technology – Paraná</li>
            <li><strong>Andrea Vestrucci</strong>, University of Bamberg</li>
            <li><strong>Yasmeen Rafiq</strong>, University of Manchester</li>
        </ul>
    </div>

    <div class="horizontal-rule"></div>
    <div id="footer">
        <div class="copyright">© FEAR 2025</div>
        ⋅
        <div class="logos"><img src="/media/uom.png" title="University of Manchester" />
            <img src="/media/rae.png" title="Royal Academy of Engineering" />
            <img src="/media/kcl.png" title="King’s College London" />
        </div>
    </div>
</body>

</html>